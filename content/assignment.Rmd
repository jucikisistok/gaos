---
title: "Assignment"
author: Judit Kisist??k
date: '2018-05-26'
slug: assignment
summary: GWAS SNP project. Identifying patterns using odds ratios, binomial and chi^2 tests and Fisher's exact test. Simulating null distributions.
credit: Made by Judit Kisist??k. The R exercise session was created by Palle Villesen and Thomas Bataillon at Aarhus University.
categories: []
tags: []
---
###<a name="toc"></a>Table of contents
1. [Table of contents](#toc)
2. [Learning goals](#goals)
3. [GWAS project](#assignment)
    * [Looking at the data](#part1)
    * [Odds ratios for one contingency table](#part2)
    * [Many odds ratios](#part3)
    * [Look for a biological signal in the genome using the odds ratios](#part4)
    * [Chi^2 calculations](#part5)
    * [Correction for continuity](#part6)
    * [Simulating/permuting the null distribution](#part7)
    * [Trying Fisher's exact test](#part8)
    * [Making a Manhattan plot and correcting for multiple testing](#part9)
    * [The distribution of the significant SNPs in the genome](#part10)

###<a name="goals"></a>Learning goals
####By the end of this session, you should be able to...
+ calculate odds ratios by hand and associated values (confidence intervals etc.)
+ visualize odds ratios
+ identify extreme odds ratios
+ calculate $chi^2$ statitics by hand and use $chi^2$ distribution to get a p-value
+ simulate a null distribution to get a p-value
+ use R's $chi^2$ test function to get a p-value
+ use R's Fisher's exact test to get a p-value
+ compare the results
+ select a method and test all 280.000 SNPs
+ identify interesting patterns
+ explain the pattern by biomedical knowledge

```{r message = FALSE, warning = FALSE}
library(tidyverse)
library(microbenchmark)
```


###<a name="assignment"></a><u>GWAS project</u>

First, we read the data and see what kind of variables we have.

```{r}
snpdata = readRDS(file = "gwas_allele_counts.julia.2008.rds")
head(snpdata)
```

This dataset is described in the paper "Genome-wide association study of rheumatoid arthritis in the Spanish population: KLF12 as a risk locus for rheumatoid arthritis susceptibility" by Julia et al.

The first three columns are easy:  

<b>rsid </b>               = SNP id  
<b>chromosome  </b>        = the name of the chromosome  
<b>chromosome_position</b> = the position of the SNP within the chromosome  

We have taken all the chromosome and glues them together from chromosome 1...chromosome 22, then we made a new coordinate system starting from 1 (chromosome 1) and ending in position ~3 gigabases (chromosome 22). This position is calculated in the column called "<b>genome_position</b>". Finally we have counted the number of alleles in cases and controls and put this contingency table into the variable "<b>contingency_table</b>".

###<a name="part1"></a>Looking at the data

#### Count the number of SNPs per chromosome.
```{r}
table(snpdata$chromosome)
```

####Plot the number of SNPs per chromosome.
```{r}
barplot(table(snpdata$chromosome), col = "deeppink4")
```

We can see that the number of SNPs shows a decreasing tendency - this is likely due to the fact that the chromosomes have different lengths, and the longer the chromosome is, the more likely it is to have more SNPs on it.

Now we will look at the position of SNPs in the genome in more detail.
```{r}
hist(snpdata$chromosome_position, col = "deeppink4", 
     xlab = "Chromosome position",
     main="Position of SNPs in the genome") 
```

This tendency can also be explained by the chromosome size. All chromosomes are at least a few million bp long, but only the longest ones stretch to, say, $1.5*10^8$ bps and beyond.

To illustrate this, let's make a table of the chromosome positions per chromosome and make a plot.
```{r}
x = table(snpdata$chromosome, snpdata$chromosome_position%/%10^7)
barplot(x, col=rainbow(25), xlab="Chromosome", ylab="Chromosome position / 10^7") 
```

#### Make a histogram of the genome_position. First calculate the number of breaks needed so each bin is ~10 million bp, then make the histogram using that number of breaks.
```{r}
nbreaks = round(max(snpdata$genome_position) / 10^7)
hist(snpdata$genome_position, col="deeppink4", breaks = nbreaks, border = NA,
     xlab = "Genome position", main = "Histogram of genome position")
```

#### What is the average number of SNPs pr. 10 megabases? Eyeball it from the histogram.

Approximately 1000.

```{r}
x = table(snpdata$genome_position %/% 10^7)
mean(x)
median(x)
hist(x, breaks=30, col="deeppink4", main="Histogram of the number of SNPs per 10 megabases",
     xlab="")
```
  
  
*Take me back to [the beginning!](#toc)*

###<a name="part2"></a>Odds ratios for one contingency table

We will look at SNP number 1 and SNP number 2059.

```{r}
M  = snpdata$contingency_table[[1]] # extracting the contingency table corresponding to SNP 1
M2 = snpdata$contingency_table[[2059]]

M
addmargins(M) # adding the row and column sums 
M2
addmargins(M2)

a = M[1,1] # getting the value from the first row and first column
c = M[2,1] # getting the value from the second row and first column
a+c
```

#### Use the equations from the book the calculate the odds ratio for the first table (M). Make 4 variables like the layout in example 9.3 - also page 239 in Analysis of biological data, 2nd edition. So you should make a,b,c,d from the contingency table.

```{r}
a = M[1,1]
b = M[1,2]
c = M[2,1]
d = M[2,2]
OR = (a*d)/(b*c)
OR
```


#### Use the equations from the book the calculate the SE of the odds ratio.
```{r}
SE = sqrt(1/a + 1/b + 1/c + 1/d)
SE
```

#### Use the equations from the book the calculate the 99% confidence interval of the odds ratio.
```{r}
low99  = exp(log(OR) - 2.58 * SE)
high99 = exp(log(OR) + 2.58 * SE)
low99
high99
```

#### Does the confidence interval overlap 1.0? What is your conclusion?

Yes, the confidence interval overlaps 1.0. Based on these results, we can't conclude that there is an association between our variables - there might be a slight positive or slight negative effect, or no effect at all, so more analysis would be needed to pinpoint what exactly is going on.

####Also calculate the OR and the 99% CI for M2 (the other SNP).
```{r}
a = M2[1,1]
b = M2[1,2]
c = M2[2,1]
d = M2[2,2]
OR = (a*d)/(b*c)
SE = sqrt(1/a + 1/b + 1/c + 1/d)
low99  = exp(log(OR) - 2.58 * SE)
high99 = exp(log(OR) + 2.58 * SE)
cat(low99, "<", OR, "<", high99, "\n")
```

#### Does the confidence interval overlap 1.0? What is your conclusion?

No, in this case the confidence interval for the odds ratio exceeds 1, which suggests an association between the variables (so an association between having the SNP and having the disease, in this case, rheumatoid arthritis).

#### Make a plot similar to figure 9.3-1 for SNP 1 and SNP 2059.

```{r}
mosaicplot(snpdata$contingency_table[[1]], col=c("firebrick", "goldenrod1"), cex.axis = 1.2, 
           sub = "Allele", dir = c("h","v"), ylab = "Group",
           main ="Contingency table for SNP 1")

mosaicplot(snpdata$contingency_table[[2059]], col=c("firebrick", "goldenrod1"), cex.axis = 1.2, 
           sub = "Allele", dir = c("h","v"), ylab = "Group", main=paste("SNP 2059\n", 
          round(low99,2), "<", round(OR,2), "<", round(high99,2)))
```

We can also perform Fisher's exact test for these SNPs.
```{r}
fisher.test(M)
fisher.test(M2)
```

As expected, we get a high p-value for SNP 1, which makes sense as the confidence interval for this SNP overlapped 1 so we can't reject the null hypothesis of no association (or OR being 1). In contrast, however, the p-value for SNP 2059 is below 0.05, so we can reject the null hypothesis of no association. These results are visually supported by the mosaic plots.

*Take me back to [the beginning!](#toc)*

###<a name="part3"></a>Many odds ratios

#### Calculate the odds ratio and 99% confidence intervals for all contigency tables. Add these to the snpdata and call them oddsratio, low99 and high99.

```{r}
# first, we are making three vectors which are as long as many rows our SNPdata dataframe has - so as many SNPs we have in total
r1 = rep(NA, nrow(snpdata))   
r2 = rep(NA, nrow(snpdata))   
r3 = rep(NA, nrow(snpdata))   

# then we loop through the entire dataset
for (i in 1:length(r1)) {
  
  # we extract the i-th contingency table
  M = snpdata$contingency_table[[i]]
  
  # then we make our calculations on this contingency table
  a = M[1,1]
  b = M[1,2]
  c = M[2,1]
  d = M[2,2]
  
  OR     = (a*d)/(b*c)
  SE     = sqrt(1/a + 1/b + 1/c + 1/d)
  low99  = exp(log(OR) - 2.58 * SE)
  high99 = exp(log(OR) + 2.58 * SE)
  
  # then we put the calculated values to the i-th position of our vectors
  r1[i] = OR
  r2[i] = low99
  r3[i] = high99
}

# finally, we append these vectors as new columns to our dataset
snpdata$oddsratio = r1
snpdata$low99 = r2
snpdata$high99 = r3
```

#### Plot the odds ratio as function of genome position for all odds ratios.
```{r}
plot(oddsratio ~ genome_position, data = snpdata, col = chromosome,
     ylab = "Odds ratio", xlab = "Genome position",
     main = "OR as function of genome position, colored by chromosome") 
```

#### Plot the odds ratio as function of genome position for all odds ratios where the 99% confidence interval doesn't overlap 1.00.

In this case, we want a subset of the data where either the lower bound for the confidence interval (low99) is above 1 or the upper bound (high99) is below 1, which are the conditions that we specify in the subset function. The | symbol between them means "or", so we get all the datapoints where the CI spans either below or above 1. If we substituted | for &, that would mean "and" and we would get no datapoints as our conditions are mutually exclusive.

```{r}
plot(oddsratio  ~ genome_position, data = subset(x=snpdata, subset = low99 > 1.00 | high99 < 1.00),
     col = chromosome, ylab = "Odds ratio", xlab = "Genome position",
     main = "OR as function of genome position, colored by chromosome")  
```

A gentle introduction to ggplot - it is an entirely different way of plotting in R, using the package "ggplot2" from the "tidyverse". It's pretty awesome, so if you're interested in plotting, definitely look it up.

```{r warning=FALSE}
plot1 = ggplot(data = snpdata %>% filter(low99 > 1.0 | high99 < 1.0)) +
  geom_point(mapping = aes(x=genome_position, y=oddsratio, color=factor(chromosome))) +
  ggtitle("OR as function of genome position, colored by chromosome") +
  xlab("Genome position") +
  ylab("Odds ratio")

plot(plot1)
```

#### Count the number of SNPs where the 99% confidence interval of the odds ratio doesn't overlap 1.00.

```{r}
sum(snpdata$low99 > 1.0 | snpdata$high99 < 1.0)
```


*Take me back to [the beginning!](#toc)*

###<a name="part4"></a>Look for a biological signal in the genome using the odds ratios

#### Make a subset of the SNPs where the 99% confidence interval of the odds ratio doesn't overlap 1.00 and count the number of these SNPs on each chromosome.

```{r}
df = subset(x = snpdata, subset = low99 > 1.0 |  high99 < 1.0)
x = table(df$chromosome)
x
```

#### Which chromosome have the highest percentage of SNPs where the 99% confidence interval do NOT overlap 1.00?
E.g. it turns out that 129 SNPs on chromosome 8 has a 99% CI above or below 1.0. There are 16470 SNPs on chromosome 8 in total, so 129 / 16470 is 0.78%.

```{r}
x = table(df$chromosome) 
y = table(snpdata$chromosome) 
sort(x/y) 

barplot(sort(x/y), col = "deeppink4", main ="Percentage of SNPs where the 99% CI doesn't overlap 1")
```

We can see that chromosome 6 has the highest percentage of such SNPs.

#### Extract the significant SNPs from this chromosome and plot the odds ratio as function of chromosome_position.
```{r}
df = subset(x = snpdata, subset = chromosome==6 & (low99 > 1.0 |  high99 < 1.0)) 
plot(oddsratio  ~ chromosome_position, data = df, col = "deeppink4", 
     ylab = "Odds ratio", xlab = "Genome position", 
     main = "OR as function of genome position")  

ggplot(data = df) + 
  geom_point(mapping = aes(x=chromosome_position, y=oddsratio), col = "deeppink4") + 
  geom_hline(yintercept = 1) +
  ggtitle("OR as function of genome position") +
  ylab("Odds ratio") +
  xlab("Genome position")

```

#### Do the SNPs with a CI above or below 1.0 look like they are randomly distributed or clumped?

They seem to cluster together around 30 megabases.
```{r}
table(df$chromosome_position %/% 1000000)
```
We see 39 SNPs between 31 and 32 mb.

*Take me back to [the beginning!](#toc)*

###<a name="part5"></a>Chi^2 calculations

Let's look at SNP number 2059.

```{r}
Observed = snpdata$contingency_table[[2059]] # we extract the contingency table

# then we extract the values we need from the contingency table
a = Observed[1,1]
b = Observed[1,2]
c = Observed[2,1]
d = Observed[2,2]

n = a+b+c+d # number of observations in total

# calculating the probabilities for the rows and columns
Pr_row1 = (a+b)/n 
Pr_row2 = (c+d)/n
Pr_col1 = (a+c)/n
Pr_col2 = (b+d)/n

# calculating the expected counts for each cell
E_a = Pr_row1 * Pr_col1 * n 
E_b = Pr_row1 * Pr_col2 * n 
E_c = Pr_row2 * Pr_col1 * n 
E_d = Pr_row2 * Pr_col2 * n 

# then we create a table from our calculated values
# c() concatenates the two numbers
# rbind() "glues together" the two rows of those concatenated numbers
# as.table() converts this structure into a table
Expected = as.table(rbind(c(E_a, E_b), c(E_c, E_d)))
colnames(Expected) = c("allele1", "allele2")
rownames(Expected) = c("cases", "controls")

mosaicplot(Observed, col=c("firebrick", "goldenrod1"), cex.axis = 1.2, 
           sub = "Allele", dir = c("h","v"), ylab = "Group", main="SNP 2059 - observed")

mosaicplot(Expected, col=c("firebrick", "goldenrod1"), cex.axis = 1.2, 
           sub = "Allele", dir = c("h","v"), ylab = "Group", main="SNP 2059 - expected")

# then we can do the entire chi^2 test by doing calculations with our tables
# this is going to subtract the corresponding cell values:
Observed-Expected 

# so we can calculate chi^2 accordingly:
chisquare = sum((Observed-Expected)^2/Expected)
df = (nrow(Observed)-1)*(ncol(Observed) - 1) # Page 249 in Analysis of biological data, 2nd edition

p = pchisq(q = chisquare, df = df, lower.tail = FALSE)
cat("X-square:", chisquare, "df =", df, "p =", p, "\n")

#### Built-in test in R ####
chisq.test(Observed, correct = F)
```

#### Compare the values of chi^2, the p-value and the degrees of freedom for the two approaches.

Apart from differences due to rounding, they are the same.

*Take me back to [the beginning!](#toc)*

###<a name="part6"></a>Correction for continuity

You can find information about this on page 251 in Analysis of biological data, 2nd edition.

#### Perform a chi^2 test with correction for continuity, and also do it with the built-in chisq.test function. Compare the results.
```{r}
# notice that the formula for chi^2 changes a bit - we subtract 1/2 and we also take the absolute value
chisquare = sum((abs(Observed-Expected)-1/2)^2/Expected) 
df = (nrow(Observed)-1)*(ncol(Observed) - 1) 
p = pchisq(q = chisquare, df = df, lower.tail = FALSE)

cat("X-square:", chisquare, "df =", df, "p =", p, "\n")

# Compare with the built-in R version of continuity correction - we just need to specify correct = T
chisq.test(Observed, correct = T)
```

We get the same results.

*Take me back to [the beginning!](#toc)*

###<a name="part7"></a>Simulating/permuting the null distribution

```{r}
# we can start by creating a new data structure
M = rbind(rbind(rbind(
  # we create as many rows as many cases we had with Allele 1
  data.frame("Group"=rep("cases", a),    "Allele" = rep("1", a)),
  # then as many controls we had with Allele 2
  data.frame("Group"=rep("controls", c), "Allele" = rep("1", c))),
  # and do the same with the cases and controls having Allele 2
  data.frame("Group"=rep("cases", b),    "Allele" = rep("2", b))),
  data.frame("Group"=rep("controls", d), "Allele" = rep("2", d)))

# now we have a data frame with one row per individual
head(M)

# we can turn it into a table and we get the original data back
addmargins(table(M))

# let's randomly shuffle one of the columns in our data frame - the sample() function will do this for us
M$Group = sample(M$Group)
head(M)
addmargins(table(M))

# let's do this again - we get a different outcome due to the randomness of the shuffling
M$Group = sample(M$Group)
head(M)
addmargins(table(M))

# this is quite troublesome - R has a simpler way of doing this using the r2dtable() function
# we just need the row and column sums
rs = rowSums(Observed)
cs = colSums(Observed)
# and we give these values to the r2dtable() function, along with the number of simulations we need, so in this case, 5
r2dtable(n=5, r = rs, c = cs)

# so now we just do this one million times
simulations = 1000000
# Simulate a lot of tables
set.seed(0)
simulated_tables = r2dtable(n = simulations,r = rs, c = cs)
# Check the first two tables
addmargins(simulated_tables[[1]])
addmargins(simulated_tables[[2]])
addmargins(Observed)

# now we want to do a chi^2 test for all of them
# first we create a vector that is as long as many simulations we have
sim_chisquares = rep(NA, simulations)

# then we loop through all the tables and calculate the chi square for each of them
for (j in 1:simulations) {
  sim_chisquares[j] = sum(((simulated_tables[[j]]-Expected)^2)/Expected)
}

# let's calculate the observed chi^2 as well
chisquare = sum((Observed-Expected)^2/Expected)
df = (nrow(Observed)-1)*(ncol(Observed) - 1) 

# now, for plotting the theoretical chi^2 distribution, we create breaks that go from 0 to the maximum value of our simulated chi^2 values, by 0.5
breaks = seq(0,ceiling(max(sim_chisquares)), by = 0.5)
# and calculate the midpoints for each bin
midpoints = 1/2 * (breaks[2:length(breaks)] + breaks[1:(length(breaks)-1)])

# we basically calculate the expected frequency of chi^2 values between 0 and 1 - and multiply by the total number of simulations
# we have the midpoints on the x axis
x = midpoints 
# and the probabilities on the y axis
y = (pchisq(q = breaks[2:length(breaks)], df=1) - pchisq(q=breaks[1:(length(breaks)-1)], df=1))*length(sim_chisquares)

# now we put the simulated chi^2 values on a histogram
hist(sim_chisquares, breaks=breaks, xlim = c(0, 40), col = "deeppink4",
     main = "Theoretical chi^2 distribution", xlab = "Simulated chi^2 values")
# add line with the observed value
abline(v=chisquare)
# add the real chi square distribution
lines(y ~ x, lwd=2)
```

#### How many time did the null distribution give something that was equally or more extreme than the observed?
```{r}
sum(sim_chisquares >= chisquare)
```

#### What is the estimated p-value?
```{r}
sum(sim_chisquares >= chisquare) / length(sim_chisquares)

```

#### Compare it with the built-in function that uses simulation.
```{r}
chisq.test(Observed, correct = F, simulate.p.value = TRUE, B = 1000000)
```

The chi^2 value is very close to the one we obtained and we also get a very small p-value, so we can arrive to the same conclusion using both methods.

*Take me back to [the beginning!](#toc)*

###<a name="part8"></a>Trying Fisher's exact test

```{r}
fisher.test(Observed)
x = fisher.test(Observed) # we store the results of the test in a variable
names(x) 
x$conf.int 
x$estimate # estimate for the true odds ratio
x$p.value
```

#### What about the speed of the different solutions? 
```{r}
microbenchmark(chisq.test(Observed))
microbenchmark(chisq.test(Observed, correct = F))
microbenchmark(chisq.test(Observed, simulate.p.value = TRUE))
microbenchmark(chisq.test(Observed, simulate.p.value = TRUE, B = 100000))
microbenchmark(fisher.test(Observed))
```

#### What method would you use for all 280.000 SNPS?

In itself, the chisq.test is fastest, so it's a good solution when we can make sure that we can satisfy the assumptions of the chi^2 test. However, Fisher's exact test wins if we compare it to doing a chi^2 test with simulation.

#### Test all 280.000 SNPs using Fisher's exact test. For each test you want to extract the p-value and finally save it in column snpdata$p.value.

```{r}
x = rep(NA, nrow(snpdata))

for (i in 1:length(x)) {
  result =fisher.test(snpdata$contingency_table[[i]])
  x[i] = result$p.value
}

snpdata$p.value = x
```

*Take me back to [the beginning!](#toc)*

###<a name="part9"></a>Making a Manhattan plot and correcting for multiple testing

#### Make a plot with x = genome position and y = -log10(p.value).
```{r}
plot(x = snpdata$genome_position, y=-log10(snpdata$p.value),
     ylab = "-log10(p-value)", xlab = "Genome position")

ggplot(data = snpdata) +
  geom_point(mapping = aes(x=genome_position, y=-log10(p.value), color=factor(chromosome))) +
  xlab("Genome position") +
  ylab("-log10(p-value)")
```

#### Make a Manhattan plot for the 1000 most significant SNPs.

```{r}
# we get the indices of the top 1000 significant SNPs and use these to plot
i = order(snpdata$p.value)[1:1000]
plot(x = snpdata$genome_position[i], y=-log10(snpdata$p.value[i]),
     ylab = "-log10(p-value)", xlab = "Genome position")
```

Let's adjust the p-values and add them to our data.

```{r}
# Bonferroni
snpdata$bonferroni = p.adjust(p = snpdata$p.value, method = "bonferroni")

# False discovery rate
snpdata$fdr = p.adjust(p = snpdata$p.value, method = "fdr")

# the different adjustment methods
p.adjust.methods
```

#### How many SNPs are significant using uncorrected p-values (alpha = 0.05)?
```{r}
sum(snpdata$p.value <= 0.05)
```

#### How many do you expect?
```{r}
0.05*nrow(snpdata)
```

#### How many SNPs are significant using Bonferroni-corrected p-values (alpha = 0.05)?
```{r}
sum(snpdata$bonferroni <= 0.05) 

# another way of doing this:
sum(snpdata$p.value <= 0.05/nrow(snpdata) )
snpdata[(snpdata$p.value <= 0.05/nrow(snpdata)),]
```
After Bonferroni correction, we only have 9 SNPs left, which is quite a drastic cut after having 9914 such SNPs without correction. However, this is a good thing - in a GWAS study, we are looking for disease-causing alleles and it's less helpful if we find thousands (especially since many of them are found significant due to the inflated type 1 error stemming from multiple testing).

#### How many SNPs are significant if we accept a false discovery rate of 0.1?
```{r}
sum(snpdata$fdr <= 0.1)
```

#### How many SNPs are significant if we accept a false discovery rate of 0.2?
```{r}
sum(snpdata$fdr <= 0.2)
```

*Take me back to [the beginning!](#toc)*

###<a name="part10"></a>The distribution of the significant SNPs in the genome

For the rest of the exercise we will focus on significant SNPs with an FDR <= 0.2 (false discovery rate).

```{r}
df = subset(x = snpdata, subset = fdr <= 0.2)
```

#### How many SNPs are there on each chromosome (in the original data)?
```{r}
table(snpdata$chromosome)
```

#### How many significant SNPs are there on each chromosome?
```{r}
table(df$chromosome) 
```

#### What is the proportion of significant SNPs on each chromosome?
```{r}
table(df$chromosome) / table(snpdata$chromosome)
```

#### Make a plot of this proportion on each chromosome.
```{r}
barplot(table(df$chromosome)/table(snpdata$chromosome), col="deeppink4")
```

#### Which chromosome is most enriched for significant SNPs?
```{r}
barplot(sort(table(df$chromosome)/table(snpdata$chromosome)), col="deeppink4")

```

Chromosome 6.

#### How many of all SNPs are on this chromosome? How many of all SNPs are one other chromosomes?

```{r}
table(snpdata$chromosome==6)
```

#### How many of the significant SNPs are on this chromosome? How many of the significant SNPs are one other chromosomes?
```{r}
table(df$chromosome==6) 
```

So a SNP can be significant or not, and it can be on chromosome 6 or not.

#### Use a binomial test to test if the number of significant SNPs on this chromosome is different than expected.

You can imagine this as a coin you toss n times - you get x number of heads (heads being landing on chromosome 6), and for each toss you expect to hit heads with probability p (which we estimate from how all the SNPs are distributed).

```{r}
# p is the probability that a random SNP is on chromosome 6
p = sum(snpdata$chromosome==6) / nrow(snpdata) 

# x is how many significant SNPs we actually observe on chromosome 6
x = sum(df$chromosome==6) 

# n is the number of significant SNPs we have in total (out of which each falls on chromosome 6 with probability p)
n = nrow(df) 

binom.test(x = x, n = n, p = p)
# we reject the null hypothesis - the number of significant SNPs on chromosome 6 indeed seems to be different than expected
```

#### Use a contingency table test instead.

```{r}
x = snpdata$chromosome==6 # being on chromosome 6 or not
y = snpdata$fdr <= 0.2 # being significant or not
# we put all this data into a table
z = table(x,y)
addmargins(z)
# and we can easily perform a chi^2 test 
chisq.test(z)

# we can also perform a Fisher's test
fisher.test(z)

```

Using all methods, we come to the same conclusions, so it seems that there are more significant SNPs falling on chromosome 6 than we'd expect.

#### Count the number of SNPs per chromosome.
```{r}
table(snpdata$chromosome)
```

#### Plot the number of SNPS per chromosome.
```{r}
barplot(table(snpdata$chromosome), col = "deeppink4") 

```

#### Which chromosome has the highest "concentration" of SNPs?
```{r}
# first of all, we create a new dataframe to store the chromosome size information
chromosome_sizes = aggregate(x = snpdata$chromosome_position, by = list(chr = unlist(snpdata$chromosome)), FUN = max)
chromosome_sizes = data.frame(chromosome_sizes)
colnames(chromosome_sizes) = c("chromosome", "size")
chromosome_sizes$size = as.numeric(chromosome_sizes$size)
chromosome_sizes$size/sum(as.numeric(chromosome_sizes$size))

# table of the number of SNPs in the chromosome
x = table(snpdata$chromosome)
x
#we divide all of them by the chromosome size, and sort them
sort(x / chromosome_sizes$size)
barplot(x / chromosome_sizes$size,  col = "deeppink4")

# we can plot the number of SNPs per chromosome
pd = as.data.frame(table(snpdata$chromosome))
names(pd) = c("Chromosome", "Frequency")
ggplot(pd, aes(x=Chromosome, y=Frequency)) + 
  geom_col(fill="deeppink4") + 
  ylab("Number of SNPs")

# and the chromosome size per chromosome as well
pd = as.data.frame(chromosome_sizes)
names(pd) = c("Chromosome", "Frequency")
ggplot(pd, aes(x=Chromosome, y=Frequency)) + 
  geom_col(fill="deeppink4") + 
  ylab("Chromosome size")

# finally, the SNP "concentration" - so the number of SNPs divided by the chromosome size
pd = as.data.frame(x / chromosome_sizes$size)
names(pd) = c("Chromosome", "Frequency")
ggplot(pd, aes(x=Chromosome, y=Frequency)) + 
  geom_col(fill="deeppink4") + 
  ylab("Density of SNPs")

```

Chromosome 18 is the one with the largest concentration of SNPs.

#### Test the null hypothesis that says: "The number of SNPs per megabase is the same for this chromosome and the rest of the genome.

$H_A:$ The number of SNPs per megabase on this chromosome is NOT the same as the rest of the genome

We can do a binomial test to test this null hypothesis.
```{r}
# we get the size of chromosome 18
size18 = chromosome_sizes$size[chromosome_sizes$chromosome==18]

# then the total size of the genome
totalsize = sum(chromosome_sizes$size)

# p will be the proportion of the genome that is chromosome 18
p = size18 / totalsize

# x is the number of SNPs we observe on chromosome 18
x = sum(snpdata$chromosome==18)

# n is the total number of SNPs
n = nrow(snpdata)

p*n  # expected number of SNPs on chromosome 18 - it's less than what we observe

# with these, we can do a binomial test
binom.test(x = x, n = n, p = p)
```

We reject the null hypothesis, the number of SNPs per megabase on this chromosome is not the same as the rest of the genome.

Actually, we cherry pick a little bit above. The real null hypothesis should be:
$H_0:$ The number of SNPs per base is the same for all chromosomes
$H_A:$ The number of SNPs per base is NOT the same for all chromosomes

#### Test this null hypothesis.

We can use a chi^2 test.
```{r}
# the number of SNPs per chromosome
snps = table(snpdata$chromosome)

# not SNPs - we just subtract the number of SNPs (as each have a length of 1) from the total length of the chromosome
not_snps = chromosome_sizes$size - snps

# we put these together in a table
z = rbind(snps, not_snps)
round(addmargins(z)/1000, 2)

# and do a chi^2 test
r = chisq.test(z)
r
# we can also check what would be the expected number of SNPs and not SNPs per chromosome
r$expected
```

We reject the null hypothesis, the number of SNPs per base is not the same for all chromosomes.

*Take me back to [the beginning!](#toc)*
