---
title: "Week 4: Probability"
author: Judit Kisistok
date: '2018-02-21'
slug: week-4
summary: Calculating probabilities by hand. Deriving the expectation and variance of random variables.
credit: Made by Judit Kisistók. Supplemented and formatted by Simon Grund Sørensen. The exercise session was created by Thomas Bataillon and Palle Villesen at Aarhus University.
categories: []
tags: []
---



<div id="a.-table-of-contents" class="section level3">
<h3><a name="toc"></a>A. Table of contents</h3>
<ol style="list-style-type: decimal">
<li><a href="#toc">Table of contents</a></li>
<li><a href="#goals">Learning goals</a></li>
<li><a href="#exercises">Exercises</a>
<ul>
<li><a href="#problemA">Problem A</a></li>
<li><a href="#problemB">Problem B</a></li>
</ul></li>
</ol>
</div>
<div id="learning-goals" class="section level3">
<h3><a name="goals"></a>Learning goals</h3>
<div id="by-the-end-of-this-session-you-should-be-able-to" class="section level4">
<h4>By the end of this session, you should be able to…</h4>
<ul>
<li>Calculate probabilities by hand</li>
<li>Calculate expectation and variance of a random variable</li>
<li>Calculate the expectation and variance of a sum of n random variables and relate this to the terms:
<ul>
<li>SD of a population</li>
<li>SE of a sample</li>
</ul></li>
</ul>
</div>
</div>
<div id="exercises" class="section level3">
<h3><a name="exercises"></a><u>Exercises</u></h3>
<div id="problem-a" class="section level4">
<h4><a name="problemA"></a>Problem A</h4>
</div>
<div id="a1." class="section level4">
<h4>A1.</h4>
<p><em><b><span class="math inline">\(X\)</span></b> and <b><span class="math inline">\(Y\)</span></b> are independent random variables and <b><span class="math inline">\(a\)</span></b> and <b><span class="math inline">\(b\)</span></b> are constants. You have the following scaled random variables:</em></p>
<p><span class="math display">\[U = X + Y\]</span> <span class="math display">\[V = a \cdot X\]</span> <span class="math display">\[W = a \cdot X + b\]</span></p>
<p><u>a) Calculate the expectation of these random variables!</u></p>
<p><span class="math display">\[E[X + Y] = E[X] + E[Y]\]</span></p>
<p><span class="math display">\[E[a \cdot X] = E[a] \cdot E[X] = a \cdot E[X]\]</span> because the expectation of a constant is the constant itself</p>
<p><span class="math display">\[E[a \cdot X + b] = E[a] \cdot E[X] + E[b] = a \cdot E[X] + b\]</span></p>
<p><u>b) Calculate the variance and relate these results to the table on page 85 (effect of scaling a statistical measurement)!</u></p>
<p><span class="math display">\[V[U] = V[X] + V[Y]\]</span></p>
<p><span class="math display">\[V[a \cdot X] = V[X] = a^2 \cdot V[X]\]</span> - the variance of the random variable is multiplied by the square of the constant</p>
<p><span class="math display">\[V[a \cdot X + b] = a^2 \cdot V[X]\]</span> - adding a constant doesn’t change the variance</p>
<p>If you look at the table on page 85, it basically shows you these exact same rules.</p>
<p><u><em>Proofs:</em></u> <span class="math display">\[
\begin{aligned}
V[X + Y] &amp;= E[(X+Y)^2] - (E[X+Y])^2 \\
&amp;= E[X^2 + 2 \cdot X \cdot Y + Y^2] - (E[X] + E[Y])^2 \\
&amp;= E[X^2] + 2 \cdot E[X \cdot Y] + E[Y^2] - ((E[X])^2 + 2 \cdot E[X] \cdot E[Y] + (E[Y]^2)) \\
&amp;= E[X^2] + 2 \cdot E[X \cdot Y] + E[Y^2] - (E[X])^2 - 2 \cdot E[X] \cdot E[Y] - (E[Y]^2) \\
&amp;= (E[X^2]-(E[X])^2) + (E[Y^2]-(E[Y])^2) + 2 \cdot (E[X \cdot Y] - E[X] \cdot E[Y]) \\
&amp;= V[X] + V[Y] + 2 \cdot Cov(X,Y)
\end{aligned}
\]</span> The <span class="math inline">\(Cov(X,Y)\)</span> part is called the covariance, and it expresses how our variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> vary together. However, in this case <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, therefore, the covariance is 0 - hence, we ultimately arrive to the formula <span class="math inline">\(V[X+Y] = V[X] + V[Y]\)</span>.</p>
<p><span class="math display">\[
\begin{aligned}
V[a \cdot X] &amp;= E[(a \cdot X)^2] - (E[a \cdot X])^2 \\
&amp;= E[a^2 \cdot X^2] - (a \cdot E[X])^2 \\
&amp;= a^2 \cdot E[X^2] - a^2 \cdot E[X]^2 \\
&amp;= a^2 \cdot (E[X^2] - (E[X])^2) \\
&amp;= a^2 \cdot V[X] 
\end{aligned}
\]</span></p>
<p><span class="math display">\[
\begin{aligned}
V[a \cdot X + b] 
&amp;= E[(a \cdot X + b)^2] - (E[a \cdot X + b])^2 \\
&amp;= E[a^2 \cdot X^2 + 2 \cdot a \cdot b \cdot X + b^2] - (a \cdot E[X] + b)^2 \\
&amp;= a^2 \cdot E[X^2] + 2 \cdot a \cdot b \cdot E[X] + b^2 - a^2 \cdot E^2[X] - 2 \cdot a \cdot b \cdot E[X] - b^2 \\
&amp;= a^2 \cdot E[X^2] - a^2 \cdot E^2[X] \\
&amp;= a^2 \cdot (E[X^2] - E^2[X]) \\
&amp;= a^2 \cdot V[X] \\
\end{aligned}
\]</span></p>
</div>
<div id="a2." class="section level4">
<h4>A2.</h4>
<p>Imagine that you have n measurements (or observations) that are independent from each other, and that each observation comes from the same underlying population. We define a random variable for each observation: <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span>,…, <span class="math inline">\(X_n\)</span>. All random variables <span class="math inline">\(X_i\)</span> are independent have the same underlying probability distribution.</p>
<p><u>Calculate <span class="math inline">\(E[M]\)</span> and discuss what this implies when we estimate the mean of a population from the sample mean.</u></p>
<p><span class="math display">\[
\begin{aligned}
E[M] &amp;= E[(X_1 ... X_n) / n] \\
&amp;= 1/n \cdot (E[X_1] + ... + E[X_n]) \\
&amp;= 1/n \cdot (\mu + ... + \mu) \\
&amp;= 1/n \cdot (n\cdot \mu) \\
&amp;= \mu
\end{aligned}
\]</span></p>
<p>This means that the expected value of the sample mean is exactly the same as the mean of the individual <span class="math inline">\(X_i\)</span> - this means that whenever we want to estimate the mean of a population parameter, calculating the mean of our sample is the best we can do.</p>
<p><u>Calculate <span class="math inline">\(V[M]\)</span> and <span class="math inline">\(sqrt(V[M])\)</span> and relate your result to the formula of the book when calculating the <span class="math inline">\(SE\)</span> of a mean in a sample with size <span class="math inline">\(n\)</span>.</u></p>
<p><span class="math display">\[
\begin{aligned}
V[M] &amp;= V[\frac{X_1...X_n}{n}] \\
&amp;= V[\frac{1}{n^2} \cdot (X_1 + X_2 ... + X_n)] \\
&amp;= \frac{1}{n^2}\cdot\mu^2_1 + \mu^2_2... + \mu^2_n \\
&amp;= \frac{1}{n^2}\cdot n \cdot \mu^2 \\
&amp; = \mu^2 / n
\end{aligned}
\]</span></p>
<p>Analogously…</p>
<p><span class="math display">\[\sqrt{(V[M])} = \frac{\mu}{\sqrt{n}}\]</span></p>
<p>Notice that this is exactly how we calculated the standard error of the mean!</p>
<p>In these proofs we were able to substitute <span class="math display">\[E[X_i] = \mu\]</span> and <span class="math display">\[V[X_i] = \sigma^2\]</span> because the observations are independent and come from the same underlying probability distribution. <b>The random variables are independently and identically distributed, so they have the same mean and variance.</b></p>
<p>Take me back to <a href="#toc">the beginning!</a></p>
</div>
</div>
<div id="problem-b" class="section level3">
<h3><a name="problemB"></a>Problem B</h3>
<p>Let <span class="math inline">\(X\)</span> be a random variable recording the genotype of an individual for a given SNP in the genome.</p>
<p>An individual is either AA, AT or TT and we map the genotype to the number of A alleles so if we have an individual being TT then <span class="math inline">\(X=0\)</span>, AT then <span class="math inline">\(X=1\)</span>, and AA then <span class="math inline">\(X=2\)</span>.</p>
<p>Imagine that we sample a very large (infinite) population and that the frequency of T in the population is 0.15. Imagine also that individuals are mating at random.</p>
<p><u>Calculate the following probabilities:</u></p>
<p><span class="math inline">\(X = 0\)</span> means that the individual has the TT genotype.<br />
<span class="math display">\[Pr(X = 0) = Pr[T] \cdot Pr[T] = 0.15 \cdot 0.15 = 0.0225\]</span></p>
<p><span class="math inline">\(X = 1\)</span> means that the individual has the AT genotype.<br />
<span class="math display">\[Pr(X = 1) = (Pr[A] \cdot Pr[T]) + (Pr[T] \cdot Pr[A]) = 0.85 \cdot 0.15 = 0.255\]</span></p>
<p><span class="math inline">\(X = 2\)</span> means that the individual has the AA genotype.<br />
<span class="math display">\[Pr(X = 2) = Pr[A] \cdot Pr[A] = 0.85 \cdot 0.85 = 0.7225\]</span></p>
<p><u>Calculate <span class="math inline">\(E[X]\)</span> and <span class="math inline">\(V[X]\)</span>.</u></p>
<p><span class="math display">\[
\begin{aligned}
E[X] &amp;= 0 \cdot Pr[X = 0] + 1 \cdot Pr[X = 1]+2 \cdot Pr[X = 2] \\
&amp;= 0 \cdot 0.0225 + 1 \cdot 0.255 + 2 \cdot 0.7225 = 1.7
\end{aligned}
\]</span></p>
<p>Fine, but what does this mean? According to this calculation the expectation of <span class="math inline">\(X\)</span> is 1.7 - however, we don’t even have this option. <span class="math inline">\(X\)</span> can be either 0, 1 or 2, corresponding to the genotypes TT, AT and AA. Our result is closest to 2, so we can say that we’d expect genotype AA to be the “typical” one. If we’re looking at the probabilities, this actually makes sense as the probability of the AA genotype is the largest, 0.7225, so we wouldn’t be very suprised to observe that genotype the most often.</p>
<p>Quick sidenote: if you think about 0, 1 and 2 as numbers here, you might be led astray because then you might start thinking that when we’re multiplying <span class="math inline">\(Pr[X = 0]\)</span> by 0, we’re essentially “losing” or “disregarding” it. This is not true. Let’s imagine a population where there are <em>only</em> TT individuals (so no A alleles whatsoever). Then the calculation would look like this:</p>
<p><span class="math display">\[Pr(X = 0) = Pr[T] \cdot Pr[T] = 1 \cdot 1 = 1\]</span> <span class="math display">\[Pr(X = 1) = (Pr[A] \cdot Pr[T]) + (Pr[T] \cdot Pr[A]) = 1 \cdot 0 = 0\]</span></p>
<p><span class="math display">\[Pr(X = 2) = Pr[A] \cdot Pr[A] = 0 \cdot 0 = 0\]</span> <span class="math display">\[
\begin{aligned}
E[X] &amp;= 0 \cdot Pr[X = 0] + 1 \cdot Pr[X = 1]+2 \cdot Pr[X = 2] \\
&amp;= 0 \cdot 1 + 1 \cdot 0 + 2 \cdot 0 = 0
\end{aligned}\]</span> Does this mean that we calculated that the expectation is <em>nothing</em>? No. What we calculated is that the expectation is 0, corresponding to genotype TT, meaning that the typical individual has the TT genotype, which makes sense if there are only T alleles in the population. So in this case think about the values of <span class="math inline">\(X\)</span> as “indicators” for the genotypes.</p>
<p>Let’s get back to the original exercise now and calculate the variance:</p>
<p><span class="math display">\[
\begin{aligned}
V[X] &amp;= E[X^2] - (E[X])^2 \\
&amp;= (0\cdot0.0225 + 1\cdot0.255 + 4\cdot0.7225) - 1.7^2 \\
&amp;= 3.145 - 2.89 = 0.255
\end{aligned}\]</span> Notice what you need to square here! <span class="math inline">\((E[X])^2\)</span> is fairly simple - you just take what you calculated in the previous exercise, square it and call it a day.<br />
<span class="math inline">\(E[X^2]\)</span> is a bit more tricky - it’s the expectation of <span class="math inline">\(X^2\)</span>. The <b>probabilities remain the same</b>, however, you need to multiply them by <span class="math inline">\(X^2\)</span> instead of <span class="math inline">\(X\)</span> (ie. 0, 1, and 4 instead of 0, 1 and 2).</p>
<p>me back to <a href="#toc">the beginning!</a></p>
</div>
